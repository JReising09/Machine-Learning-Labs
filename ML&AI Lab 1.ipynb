{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML & AI - Lab 1 - Hello, AI Agent\n",
    "\n",
    "### Justin Reising\n",
    "\n",
    "\n",
    "### Step 1 - Modeling the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Monday = [[8,9,13,15],[8,12,13,15,16],[9,10,11,13,15],[10,11,13,15],[8,9,10,13,14,15]]\n",
    "Tuesday = [[9,11,16],[8,9,11,13,14],[9,10,11],[8,9,10,11],[11,12,13,14,15]]\n",
    "Wednesday = [[8,9,10,12],[8,12,13,15,16],[8,9,10,11,13,14,15],[10,11,13,15],[8,9,10,13,14]]\n",
    "Thursday = [[10,12,13,15,16],[8,9,11,13,14],[9,10,13],[8,9,13,15],[12,13,14,15]]\n",
    "Friday = [[8,9,10,11,12,13,14,15,16],[8,9,11,13,14,15],[8,9,11,12,15],[8,9,11,13],[8,9,10,11,13]]  \n",
    "\n",
    "ConferenceRoom = [Monday[0],Tuesday[0],Wednesday[0],Thursday[0],Friday[0]]\n",
    "Yourself = [Monday[1],Tuesday[1],Wednesday[1],Thursday[1],Friday[1]]\n",
    "Anna = [Monday[2],Tuesday[2],Wednesday[2],Thursday[2],Friday[2]]\n",
    "Bob = [Monday[3],Tuesday[3],Wednesday[3],Thursday[3],Friday[3]]\n",
    "Carrie = [Monday[4],Tuesday[4],Wednesday[4],Thursday[4],Friday[4]]\n",
    "\n",
    "Daily = [Monday,Tuesday,Wednesday,Thursday,Friday]\n",
    "Loc = [ConferenceRoom, Yourself, Anna, Bob, Carrie]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2- Define the Generate and Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateAndTest(list1,list2,list3,list4,list5):\n",
    "    Daily = [list1,list2,list3,list4,list5]\n",
    "    PMT = [] #Possible Meeting Times List\n",
    "    days = {0:\"Monday\",1:\"Tuesday\",2:\"Wednesday\",3:\"Thursday\",4:\"Friday\"}\n",
    "    for a in range(len(Daily)):\n",
    "        d = Daily[a]\n",
    "        result = set(d[0])\n",
    "        for s in d[1:]:\n",
    "            result.intersection_update(s)\n",
    "        PMT.append(result)\n",
    "\n",
    "    for day in range(len(PMT)):\n",
    "        if len(PMT[day])!=0:\n",
    "            print(days[day])\n",
    "            for time in PMT[day]:\n",
    "                print('-  Potential Meeting at %d:00' % time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday\n",
      "-  Potential Meeting at 13:00\n",
      "-  Potential Meeting at 15:00\n",
      "Tuesday\n",
      "-  Potential Meeting at 11:00\n",
      "Thursday\n",
      "-  Potential Meeting at 13:00\n",
      "Friday\n",
      "-  Potential Meeting at 8:00\n",
      "-  Potential Meeting at 9:00\n",
      "-  Potential Meeting at 11:00\n"
     ]
    }
   ],
   "source": [
    "GenerateAndTest(Monday,Tuesday,Wednesday,Thursday,Friday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Agent Type\n",
    "\n",
    "This is a Goal-Based Agent because the function is trying to achieve the goal of selecting common meeting times. Through the iterative process of comparing set intersections between lists of possible meething times for each individual and the 'Conference Room'. This could also be described as a Simple Reflex Agent as the environment is fully observable with the input of available times. Also, since this agent has a single objective to return a list of possible mutual meeting times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PEAS Description\n",
    "\n",
    "\n",
    "__Performance Measure:__  Common meeting times.\n",
    "\n",
    "__Environment:__ Conference room and colleagues.\n",
    "\n",
    "__Actuators:__ Display of potential meeting times.\n",
    "\n",
    "__Sensors:__ Keyboard entry of available times for each individual and the conference room for each day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Book Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3\n",
    "For each of the following assertions, say whether it is true or false and support your answer with examples or counterexamples where appropriate.\n",
    "\n",
    "__a)__ An agent that senses only partial information about the state cannot be perfectly rational.\n",
    "\n",
    "__False__ - The agent can be perfectly rational in regards to the partial information it does recieve.\n",
    "\n",
    "__b)__ There exist task environments in which no pure reflex can behave rationally.\n",
    "\n",
    "__True__ - Since a pure reflex agent ignores precepts and cannot estimate the state optimally in a partially observable environment.\n",
    "\n",
    "__c)__ There exists a task environment in which every agent is rational.\n",
    "\n",
    "__True__ - For example, a tautological single state where it doesn't matter what action is taken.\n",
    "\n",
    "__d)__ The input to an agent program is the same as the input to the agent function.\n",
    "\n",
    "__False__- the agent program takes only the current precept while the agent function in the entire precept sequence to a point. (So true at the state of the initial precept I suppose.)\n",
    "\n",
    "__e)__ Every agent function is implementable by some program/machine combination.\n",
    "\n",
    "__Fasle__ - An agent function that requires solving intractable problem instances.\n",
    "\n",
    "__f)__ Suppose an agent selects its action uniformly at random from a set of possible actions. There exists a determinisitc task environment in which this agent is rational.\n",
    "\n",
    "__True__- Because a uniform set of possible actions to take is still \"deterministic\": selecting randomly is still rational.\n",
    "\n",
    "__g)__ It is possible for a given agent to be perfectly rational in two distinct task environments.\n",
    "\n",
    "__True__- We can arbitraily modify parts of the environment that are unreachable by any optimal policy.\n",
    "\n",
    "\n",
    "__h)__ Every agent is rational in an unobservable environment.\n",
    "\n",
    "__False__- Some actions are \"unfavorable\" regardless of the percieved environment state.\n",
    "\n",
    "__i)__  A perfectly rational poker-playing agent never loses.\n",
    "\n",
    "__False__- Unless the agent has a Royal Flush, otherwise the agnet can always lose (or at least not win outright) given any other hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4\n",
    "For each of the following activities, give the PEAS description of the task environment and characterize it in terms of the properties listed in Section 2.3.2.\n",
    "\n",
    "__a) Playing Soccer__ - __P:__ Winning the game. __E:__ Soccer Field. __A:__ Players and ball. __S:__ Camera, Radar, Physical Sensors on Players, Manual score keeping, etc.\n",
    "\n",
    "__Task Environment:__ Partially observable, stochastic, dynamic, constinuous, multi-agent.\n",
    "\n",
    "__b) Exploring Suface of Titan__ - __P:__ Surface conditions. __E:__ Titan Surface. __A:__ Titan topological variances (Slopes, cliffs, ice, land, etc.) __S:__ Cameras, Radar, meteorological sensors.\n",
    "\n",
    "__Task Environment:__ Partially observable, stochastic, dynamic, constinuous, single agent (Most Likely).\n",
    "\n",
    "__c) Shopping for used AI books__- __P:__ Cost of book. __E:__ Online/Physical bookstore. __A:__ Book listings, physical obstructions (stairs, bookshelves, etc). __S:__ Display of book listings.\n",
    "\n",
    "__Task Environment:__ Partially observable, deterministic, sequential, static, discrete, single-agent.\n",
    "\n",
    "__d) Playing a Tennis Match__ - __P:__ Winning the match. __E:__ Tennis court. __A:__ Players, ball, raquet. __S:__ Camera, Radar, manual scoring.\n",
    "\n",
    "__Task Environment:__ Fully Observable, stochastic, episodic, dynamic, continuous, multi-agent.\n",
    "\n",
    "__e) Practicing tennis against wall:__ - __P:__ Striking ball. __E:__ Room, or wall area. __A:__ Player, wall, raquet, robot arm (to return hit ball). __S:__ Camera . \n",
    "\n",
    "__Task Environment:__ Fully Observable, stochastic, episodic, dynamic, continuous, single agent.\n",
    "\n",
    "__f) Performing High Jump:__ - __P:__ Clearing the horizontal pole. __E:__ Track & Field Arena. __A:__ Pole, person (or robot maybe hydrolics to initiate jump) jumping. __S:__ Camera, instrument measuring height.\n",
    "\n",
    "__Task Environment:__ Fully Observable, stochastic, sequential, static, continuous, single agent.\n",
    "\n",
    "__g) Knitting a Sweater:__ - __P:__ Successful 'knit'. __E:__ Yarn, Knitting 'tools'. __A:__ Robotic arms/hands. __S:__ Camera, locomotive sensors.\n",
    "\n",
    "__Task Environment:__ Fully observable, deterministic, sequential, static, continuous, single agent.\n",
    "\n",
    "__f) Bidding on Item in Auction:__ - __P:__ Winning the bid. __E:__ Auction. __A:__ Auctioneer (Could be virtual). __S:__ Audio input, microphone, etc.\n",
    "\n",
    "__Task Environment:__ Fully Observable, strategic, sequential, static, discrete, multi-agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5\n",
    "Define in your own words the following terms:\n",
    "\n",
    "__Agent:__ An entity that can percieve and/or act.\n",
    "\n",
    "__Agent Function:__ A function that specfies action in response to percept sequence.\n",
    "\n",
    "__Agent Program:__ A program that implements the agent function.\n",
    "\n",
    "__Rationality:__ A property of agents that choose actions to maximize their expected utility.\n",
    "\n",
    "__Autonomy:__ A property of agents whose behavior is determined by it's experience.\n",
    "\n",
    "__Reflex Agent:__ An agent whose action responds only to the current precept.\n",
    "\n",
    "__Model-Based Agent:__ An agent whose actions are guided by internal models.\n",
    "\n",
    "__Goal-Based Agent:__ An agent whose actions are chosen to achieve a goal explicitly. \n",
    "\n",
    "__Utility_Based Agent:__ An agent whose actions are chosen to maximize utility of outcome state.\n",
    "\n",
    "__Learning Agent:__ An agent whose behavior improves over time and experience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
